# NearYou ID - Testing Strategy

**Version:** 1.1  
**Last Updated:** 2025-10-17  
**Status:** Active

---

## Testing Overview

This document outlines the testing strategy for NearYou ID, including unit tests, integration tests, end-to-end tests, and performance tests. The goal is to achieve >80% code coverage and ensure high-quality, reliable software.

---

## Testing Pyramid

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   E2E Tests â”‚  (10%)
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚ Integration Tests â”‚  (30%)
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚       Unit Tests            â”‚  (60%)
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Test Distribution
- **Unit Tests:** 60% - Fast, isolated, test individual components
- **Integration Tests:** 30% - Test component interactions
- **E2E Tests:** 10% - Test complete user flows

---

*(All original sections about Unit, Integration, E2E, Performance, CI/CD, etc. remain the same.)*

---

## Best Practices Reference

Before implementing or testing any feature:
1. **Verify Latest Standards**: Check official documentation for the latest API versions and best practices
2. **Official Examples**: Reference official GitHub repositories for current implementation patterns
3. **Documentation Priority**: Official docs â†’ Official examples â†’ Established community resources
4. **Stay Updated**: Regularly check for updates to frameworks and libraries used in the project

**Key Official Documentation:**
- Kotlin: https://kotlinlang.org/docs/
- Ktor: https://ktor.io/docs/
- Compose Multiplatform: https://www.jetbrains.com/lp/compose-multiplatform/
- PostgreSQL: https://www.postgresql.org/docs/
- PostGIS: https://postgis.net/documentation/

---

## Git Workflow for Testing

### Branch Strategy
**Always create a task-specific branch before any changes:**

```bash
# Create and switch to task branch
git checkout -b task/{task_id}-{description}

# Examples:
git checkout -b task/T-101-backend-auth
git checkout -b task/T-202-post-creation
```

**Never commit directly to main or develop.**

### Testing Workflow
1. Create branch â†’ Implement â†’ Test locally â†’ Create validation report
2. If all tests pass: Push branch â†’ Create PR â†’ Wait for CI
3. If tests fail: Fix issues â†’ Re-test â†’ Update validation report
4. After PR approved: Merge â†’ Delete branch

---

## Incremental Development & Testing

### Error-Free Progression Principle
Never accumulate errors across multiple files. Always verify each change immediately.

**Workflow:**
1. **Create/Modify File** â†’ Verify immediately
2. **Fix Errors** â†’ Test fix works
3. **Move to Next File** â†’ Repeat

**Verification Commands:**
```bash
# After modifying Kotlin file
./gradlew compileKotlin

# After adding dependency
./gradlew build

# After creating test
./gradlew test --tests "SpecificTest"

# After database change
psql -U user -d db -c "SELECT 1;" # Quick connection test
```

**Red Flags (Stop & Fix):**
- âŒ Compilation errors
- âŒ Missing imports
- âŒ Type mismatches
- âŒ Syntax errors
- âŒ Test failures
- âŒ Broken references

**Don't Continue If:**
- Current file has any compilation errors
- Tests related to current change are failing
- Database migration fails to apply
- Dependencies cannot be resolved

**Example Good Flow:**
```
1. Create UserRepository.kt
2. Run: ./gradlew :server:compileKotlin âœ…
3. Create UserRepositoryTest.kt
4. Run: ./gradlew :server:test --tests "UserRepositoryTest" âœ…
5. Move to next file (AuthService.kt)
```

**Example Bad Flow (Don't Do This):**
```
âŒ Create 5 files with errors
âŒ Try to run tests â†’ fails
âŒ Spend 30 minutes debugging across 5 files
```

---

## Continuous Improvement

1. **Monitor Coverage:** Track coverage trends over time
2. **Review Flaky Tests:** Identify and fix unstable tests
3. **Performance Benchmarks:** Track performance metrics
4. **Test Maintenance:** Regularly update and refactor tests
5. **Team Training:** Ensure team follows testing best practices

---

## ðŸ¤– AI vs Human Testing Responsibilities

### AI Responsibilities (Terminal Access Available)
When AI has terminal access, it should handle:
- âœ… Running test commands (`./gradlew test`, `./gradlew build`)
- âœ… Checking file existence and content
- âœ… Verifying code syntax and structure
- âœ… Running linters and formatters
- âœ… Checking database schema via SQL queries
- âœ… Verifying environment setup (Docker, services)
- âœ… Git operations (branch, commit, status checks)
- âœ… Building and compiling code
- âœ… Inspecting logs and error messages

### Human Responsibilities (Manual Steps Required)
Humans must handle:
- âš ï¸ **First-time account setup** (Google, Firebase, Twilio, SendGrid)
- âš ï¸ **OAuth flows** (Google Sign-In configuration, callbacks)
- âš ï¸ **App store submissions** (Google Play, App Store)
- âš ï¸ **First-time app launch** on physical devices
- âš ï¸ **Manual UI testing** (visual verification, UX flow)
- âš ï¸ **Online configuration** (Firebase Console, AWS Console, GCP Console)
- âš ï¸ **Payment gateway testing** (Stripe, subscription flows)
- âš ï¸ **Push notification testing** (actual device notifications)
- âš ï¸ **External service integration** verification (requires web login)

### Validation Mode Decision Matrix

| Task Type | Terminal Testable? | Validation Owner | Notes |
|-----------|-------------------|------------------|-------|
| Backend API endpoints | âœ… Yes | AI | Can use curl/http tools |
| Database operations | âœ… Yes | AI | Can use psql/SQL queries |
| Unit/Integration tests | âœ… Yes | AI | Can run test commands |
| Build & compilation | âœ… Yes | AI | Can run build commands |
| Code structure | âœ… Yes | AI | Can verify files/syntax |
| First-time app launch | âŒ No | HUMAN | Requires device/emulator UI |
| Google OAuth setup | âŒ No | HUMAN | Requires web console access |
| Push notification flow | âŒ No | HUMAN | Requires physical device |
| Payment processing | âŒ No | HUMAN | Requires payment provider access |
| Visual UI verification | âŒ No | HUMAN | Requires human eyes |
| Third-party service setup | âŒ No | HUMAN | Requires web login/config |

### Example: Task Validation Planning

**Task T-101 (Backend Auth Service):**
- âœ… AI: Build, test, verify endpoints via curl, check database
- âš ï¸ HUMAN: Verify in Postman (optional), check logs manually
- **Result:** validation_owner = HYBRID (mostly AI, human optional)

**Task T-102 (Frontend Auth Flows):**
- âœ… AI: Build, compile, verify code structure
- âš ï¸ HUMAN: Launch app, test Google Sign-In flow, verify UI
- **Result:** validation_owner = HYBRID (AI for code, human for runtime)

**Task T-303 (FCM Notifications):**
- âœ… AI: Build, verify FCM integration code
- âš ï¸ HUMAN: Configure Firebase Console, test actual notifications on device
- **Result:** validation_owner = HYBRID (AI for code, human for service setup)

---

## ðŸ§© Appendix â€” How to Validate Changes (Non-technical & Technical)

Not all tasks require running code to verify correctness. Some involve reviewing generated documents, ensuring consistency, or confirming manual steps.  
Use the appropriate validation method below depending on the task type and validation owner.

---

### A. Document-Only Validation
For tasks that modify or generate documentation only (e.g., updating specs, plans, or architecture):

- Create a validation report under `docs/TEST_REPORTS/{task_id}_VALIDATION.md` using the [Task Validation Template](../TEST_REPORTS/TASK_VALIDATION_TEMPLATE.md).
- Review all related documents for:
    - Cross-file consistency (Plan â†” Architecture â†” Spec)
    - Correct links, paths, and formatting
    - Removal of placeholders (e.g., â€œTODOâ€, â€œTBDâ€)
- Attach evidence (quotes, screenshots, timestamps)
- Mark PASS or FAIL in the report

> **validation_owner:** `AI` (for consistency and text checks)  
> **validation_owner:** `HUMAN` (for subjective or domain review)

---

### B. Command-Driven Validation
For tasks that require executing commands or running code:

- Use Gradle or tool-specific commands such as:
  ```bash
  ./gradlew test
  ./gradlew integrationTest
  ./gradlew :composeApp:connectedDebugAndroidTest
  maestro test maestro/
  k6 run k6/load_test.js
  ```
- Capture logs or reports as validation evidence
- Include the results in `docs/TEST_REPORTS/{task_id}_VALIDATION.md`
- Confirm PASS criteria defined in the task plan are met

> **validation_owner:** `AI` (if verifiable through logs/reports)  
> **validation_owner:** `HUMAN` (if manual environment or credentials required)

---

### C. Mixed Validation (HYBRID)
For tasks that involve both documentation updates and technical execution:
1. Perform document validation as in section A
2. Execute relevant commands as in section B
3. Merge evidence into one validation report
4. Indicate clearly which parts were validated by AI and which by a human reviewer
5. Update Progress Ledger and Changelog after both validations pass

---

### D. Validation Reporting
- Store all validation reports in `docs/TEST_REPORTS/`
- File naming format: `T-###_VALIDATION.md`
- Each report must include:
    - validation_owner (`AI`, `HUMAN`, or `HYBRID`)
    - who_validated (agent name or initials)
    - evidence (paths, screenshots, or code snippets)
    - result (PASS/FAIL)
    - follow-up actions if applicable

---

*This appendix is part of the validation-first discipline and complements the short meta prompt in `docs/PROMPTS/VIBECODE_SHORT_META_PROMPT.md`.*
